"""
–ì–ª–∞–≤–Ω—ã–π —É–ø—Ä–∞–≤–ª—è—é—â–∏–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã:
- –†–∞–±–æ—á–∏–π-—Ç—Ä–µ–Ω–µ—Ä (TrainingWorker)
- –ù–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–±–æ—á–∏—Ö-–∏–≥—Ä–æ–∫–æ–≤ (SelfPlayWorker)
- –û–±—â—É—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å –∏ –±—É—Ñ–µ—Ä –≤ —Ä–∞–∑–¥–µ–ª—è–µ–º–æ–π –ø–∞–º—è—Ç–∏

–û–Ω —Å–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –æ–±—â–∏–µ –æ–±—ä–µ–∫—Ç—ã –∏ —É–ø—Ä–∞–≤–ª—è–µ—Ç –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –¥–æ—á–µ—Ä–Ω–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤.
"""
import multiprocessing
import torch
import logging
import sys
import time
import os
import platform
import torch.optim as optim
from torch.optim.lr_scheduler import ExponentialLR

# --- –õ–æ–∫–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã ---
import rl_chess.config as config
from rl_chess.RL_network import ChessNetwork
from rl_chess.shared_buffer import SharedReplayBuffer
from rl_chess.self_play_worker import SelfPlayWorker
from rl_chess.training_worker import TrainingWorker
from rl_chess.inference_server import InferenceServer
import multiprocessing

IS_WINDOWS = platform.system() == "Windows"

def setup_logging():
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –≥–ª–æ–±–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ–≤."""
    log_formatter = logging.Formatter(
        '%(asctime)s [%(levelname)s] [%(processName)s] %(message)s'
    )
    
    # –û—á–∏—â–∞–µ–º –ª–æ–≥-—Ñ–∞–π–ª –¥–ª—è live-–∞–ø–¥–µ–π—Ç–æ–≤ –ø–µ—Ä–µ–¥ —Å—Ç–∞—Ä—Ç–æ–º
    live_log_file = "live_updates.log"
    if os.path.exists(live_log_file):
        os.remove(live_log_file)
    with open(live_log_file, 'w') as f:
        f.write("Live updates log started.\n")

    # –§–∞–π–ª–æ–≤—ã–π –ª–æ–≥–≥–µ—Ä, –≤ UTF-8, —á—Ç–æ–±—ã –Ω–∞–≤–µ—Ä–Ω—è–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ —Å–∏–º–≤–æ–ª—ã
    file_handler = logging.FileHandler("distributed_training.log", mode='w', encoding='utf-8')
    file_handler.setFormatter(log_formatter)

    # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –ª–æ–≥–≥–µ—Ä
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(log_formatter)

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    logging.basicConfig(level=logging.INFO, handlers=[file_handler, console_handler])

def load_checkpoint(model, optimizer, scheduler, training_step_counter, total_games_played_counter, white_wins, black_wins, draws):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–µ–∫–ø–æ–∏–Ω—Ç, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç."""
    if not os.path.exists(config.CHECKPOINT_PATH):
        logging.info("–ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ—Å—Å–∏—é –æ–±—É—á–µ–Ω–∏—è.")
        return

    try:
        logging.info(f"–ù–∞–π–¥–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {config.CHECKPOINT_PATH}. –ó–∞–≥—Ä—É–∑–∫–∞...")
        device = torch.device(config.TRAINING_DEVICE)
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞, –Ω—É–∂–Ω–æ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤ "—á–∏—Å—Ç—É—é" –º–æ–¥–µ–ª—å, –∞ –∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ç–æ–º
        is_compiled = hasattr(model, '_orig_mod')
        model_to_load = model._orig_mod if is_compiled else model
        
        checkpoint = torch.load(config.CHECKPOINT_PATH, map_location=device)
        
        model_to_load.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –æ–±—â–∏–µ —Å—á–µ—Ç—á–∏–∫–∏
        with training_step_counter.get_lock():
            training_step_counter.value = checkpoint.get('training_steps', 0)
        with total_games_played_counter.get_lock():
            total_games_played_counter.value = checkpoint.get('games_played', 0)
            
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–±–µ–¥, –µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å
        if 'white_wins' in checkpoint:
            with white_wins.get_lock(): white_wins.value = checkpoint['white_wins']
        if 'black_wins' in checkpoint:
            with black_wins.get_lock(): black_wins.value = checkpoint['black_wins']
        if 'draws' in checkpoint:
            with draws.get_lock(): draws.value = checkpoint['draws']

        logging.info(f"‚úÖ –ß–µ–∫–ø–æ–∏–Ω—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω. –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å —à–∞–≥–∞ {training_step_counter.value}, —Å—ã–≥—Ä–∞–Ω–æ –∏–≥—Ä: {total_games_played_counter.value}.")
        logging.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: –ë–µ–ª—ã–µ: {white_wins.value}, –ß–µ—Ä–Ω—ã–µ: {black_wins.value}, –ù–∏—á—å–∏: {draws.value}")

    except Exception as e:
        logging.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç: {e}. –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è.")

def main():
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã.
    """
    setup_logging()
    logging.info("Starting distributed training system with Shared Model and Memory Buffer...")

    try:
        # –ù–∞ Windows –∏—Å–ø–æ–ª—å–∑—É–µ–º spawn, –Ω–∞ Linux/WSL –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –º–µ—Ç–æ–¥ (fork),
        # —á—Ç–æ–±—ã torch.compile –º–æ–∂–Ω–æ –±—ã–ª–æ –±–µ–∑–æ–ø–∞—Å–Ω–æ –ø—Ä–∏–º–µ–Ω—è—Ç—å –±–µ–∑ –ø—Ä–æ–±–ª–µ–º —Å pickling.
        if IS_WINDOWS:
            multiprocessing.set_start_method('spawn')
    except RuntimeError:
        pass

    total_games_played_counter = multiprocessing.Value('i', 0)
    training_step_counter = multiprocessing.Value('i', 0)
    
    # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–±–µ–¥ (–≥–ª–æ–±–∞–ª—å–Ω–∞—è) ---
    white_wins = multiprocessing.Value('i', 0)
    black_wins = multiprocessing.Value('i', 0)
    draws = multiprocessing.Value('i', 0)
    
    # --- –û—á–µ—Ä–µ–¥–∏ –¥–ª—è Inference Server ---
    # –û–±—â–∞—è –æ—á–µ—Ä–µ–¥—å –¥–ª—è –≤—Ö–æ–¥—è—â–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç –≤—Å–µ—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤
    input_queue = multiprocessing.Queue()
    
    num_workers = 8  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—á–∏—Ö-–∏–≥—Ä–æ–∫–æ–≤ (–±–æ–ª—å—à–µ = –±—ã—Å—Ç—Ä–µ–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è)
    # –õ–∏—á–Ω—ã–µ –æ—á–µ—Ä–µ–¥–∏ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ—Ä–∫–µ—Ä–∞
    output_queues = [multiprocessing.Queue() for _ in range(num_workers)]
    
    # --- Shared Memory Buffer –¥–ª—è –ò–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (Zero-Copy) ---
    # –†–∞–∑–º–µ—Ä: [num_workers, config.MCTS_BATCH_SIZE, config.INPUT_CHANNELS, 8, 8]
    # –ö–∞–∂–¥—ã–π –≤–æ—Ä–∫–µ—Ä –ø–∏—à–µ—Ç –≤ —Å–≤–æ–π —Å–ª–æ—Ç.
    logging.info(f"Allocating shared memory for inference: {num_workers} workers x {config.MCTS_BATCH_SIZE} batch size...")
    inference_buffer_shape = (num_workers, config.MCTS_BATCH_SIZE, config.INPUT_CHANNELS, 8, 8)
    inference_shared_buffer = torch.zeros(inference_buffer_shape, dtype=torch.float32).share_memory_()
    logging.info("Shared inference buffer allocated.")
    
    replay_buffer = SharedReplayBuffer(config.MAX_REPLAY_BUFFER_SIZE)

    # –ì–ª–æ–±–∞–ª—å–Ω—ã–µ CUDA/cuDNN –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ (–µ—Å–ª–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –∏–¥—ë—Ç –Ω–∞ GPU)
    if config.TRAINING_DEVICE == 'cuda' and torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True
        try:
            torch.set_float32_matmul_precision("high")
        except AttributeError:
            pass
        if hasattr(torch.backends.cuda, "matmul"):
            torch.backends.cuda.matmul.allow_tf32 = True
        if hasattr(torch.backends.cudnn, "allow_tf32"):
            torch.backends.cudnn.allow_tf32 = True

    model = ChessNetwork()
    
    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)
    scheduler = ExponentialLR(optimizer, gamma=config.SCHEDULER_GAMMA)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç –≤ —á–∏—Å—Ç—É—é –º–æ–¥–µ–ª—å –î–û –∫–æ–º–ø–∏–ª—è—Ü–∏–∏
    load_checkpoint(model, optimizer, scheduler, training_step_counter, total_games_played_counter, white_wins, black_wins, draws)

    # –ù–∞ Linux/WSL –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ CUDA –∏—Å–ø–æ–ª—å–∑—É–µ–º torch.compile() –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è.
    # –ù–∞ Windows –æ—Å—Ç–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –±–µ–∑ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏ –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π multiprocessing/pickling.
    if (not IS_WINDOWS) and config.TRAINING_DEVICE == 'cuda' and torch.cuda.is_available():
        logging.info("–ö–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é torch.compile() (Linux/WSL, –æ–∂–∏–¥–∞–µ—Ç—Å—è —É—Å–∫–æ—Ä–µ–Ω–∏–µ)...")
        model = torch.compile(model, mode="max-autotune")
    else:
        logging.info("–ó–∞–ø—É—Å–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –±–µ–∑ torch.compile() (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ —Å multiprocessing –Ω–∞ Windows –∏–ª–∏ –±–µ–∑ CUDA)...")

    model.share_memory() 
    logging.info("Neural network model has been moved to shared memory.")
    
    # ... –æ—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ —à–µ–¥—É–ª–µ—Ä–∞ –∏ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è –Ω–∞ GPU ...
    if training_step_counter.value > 0:
        logging.info(f"–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è LR... –ü—Ä–æ–º–∞—Ç—ã–≤–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –Ω–∞ {training_step_counter.value} —à–∞–≥–æ–≤.")
        temp_scheduler = ExponentialLR(optimizer, gamma=config.SCHEDULER_GAMMA)
        temp_scheduler.load_state_dict(scheduler.state_dict())
        optimizer.param_groups[0]['lr'] = temp_scheduler.get_last_lr()[0]
        scheduler = ExponentialLR(optimizer, gamma=config.SCHEDULER_GAMMA, last_epoch=temp_scheduler.last_epoch)
        logging.info(f"LR —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω. –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {optimizer.param_groups[0]['lr']:.8f}")

    if config.TRAINING_DEVICE == 'cuda':
        logging.info("–ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –Ω–∞ CUDA...")
        for state in optimizer.state.values():
            for k, v in state.items():
                if isinstance(v, torch.Tensor):
                    state[k] = v.cuda()

    processes = []
    try:
        # 1. –ó–∞–ø—É—Å–∫–∞–µ–º Inference Server (–¥–µ—Ä–∂–∏—Ç –º–æ–¥–µ–ª—å –Ω–∞ GPU)
        inference_server = InferenceServer(
            shared_model_state_dict=model, # –ü–µ—Ä–µ–¥–∞–µ–º —Å–∞–º—É –º–æ–¥–µ–ª—å, —Å–µ—Ä–≤–µ—Ä —Å–∫–æ–ø–∏—Ä—É–µ—Ç –≤–µ—Å–∞
            input_queue=input_queue,
            output_queues=output_queues,
            shared_inference_buffer=inference_shared_buffer
        )
        # –ü–µ—Ä–µ–¥–∞–µ–º –º–æ–¥–µ–ª—å –≤ —Å–µ—Ä–≤–µ—Ä (–∫–æ—Å—Ç—ã–ª—å –¥–ª—è multiprocessing –Ω–∞ Windows, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å shared_memory)
        inference_server.set_model(model) 
        processes.append(inference_server)
        inference_server.start()
        logging.info("Inference Server started.")

        # 2. –ó–∞–ø—É—Å–∫–∞–µ–º Training Worker (–æ–±–Ω–æ–≤–ª—è–µ—Ç –º–æ–¥–µ–ª—å)
        training_worker = TrainingWorker(
            model=model,
            # model_lock —É–±—Ä–∞–Ω, —Ç–∞–∫ –∫–∞–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –æ—Ç–¥–µ–ª–µ–Ω, –∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∞—Ç–æ–º–∞—Ä–Ω—ã –¥–ª—è shared memory (–≤ —Ç–µ–æ—Ä–∏–∏)
            # –ª–∏–±–æ –º—ã –¥–æ–ø—É—Å–∫–∞–µ–º race condition –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –≤–µ—Å–æ–≤ —Å–µ—Ä–≤–µ—Ä–æ–º, —á—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ.
            replay_buffer=replay_buffer,
            optimizer=optimizer,
            scheduler=scheduler,
            training_step_counter=training_step_counter,
            total_games_played_counter=total_games_played_counter,
            stats_counters=(white_wins, black_wins, draws) # –ü–µ—Ä–µ–¥–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        )
        processes.append(training_worker)
        training_worker.start()
        logging.info("Training Worker started.")
        
        logging.info(f"Starting {num_workers} self-play worker processes...")
        for i in range(num_workers):
            worker = SelfPlayWorker(
                worker_id=i,
                input_queue=input_queue,
                output_queue=output_queues[i],
                replay_buffer=replay_buffer,
                total_games_played_counter=total_games_played_counter,
                shared_inference_buffer=inference_shared_buffer,
                stats_counters=(white_wins, black_wins, draws) # –ü–µ—Ä–µ–¥–∞–µ–º —Å—á–µ—Ç—á–∏–∫–∏ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
            )
            processes.append(worker)
            worker.start()

        for p in processes:
            p.join()

    except KeyboardInterrupt:
        logging.info("KeyboardInterrupt signal received. Terminating all processes...")
    finally:
        for p in processes:
            if p.is_alive():
                p.terminate()
                p.join(timeout=5)
        logging.info("All processes have been successfully terminated.")

if __name__ == '__main__':
    main()