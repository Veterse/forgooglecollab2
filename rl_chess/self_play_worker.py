# -*- coding: utf-8 -*-
"""
–ü—Ä–æ—Ü–µ—Å—Å-–∏–≥—Ä–æ–∫ (Self-Play Worker).

–≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ –∏–≥—Ä–∞–µ—Ç –ø–∞—Ä—Ç–∏–∏ —Å–∞–º —Å —Å–æ–±–æ–π, –∏—Å–ø–æ–ª—å–∑—É—è –ø–æ—Å–ª–µ–¥–Ω—é—é
–≤–µ—Ä—Å–∏—é –Ω–µ–π—Ä–æ—Å–µ—Ç–∏, –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ ReplayBufferServer.
"""
import multiprocessing
import torch
import chess
import logging
import time
import os
from collections import deque

import rl_chess.config as config
from rl_chess.RL_network import ChessNetwork, board_to_tensor
from rl_chess.RL_agent import MCTSAgent
from rl_chess.RL_utils import get_live_logger, format_move, setup_worker_logging
from rl_chess.shared_buffer import SharedReplayBuffer
from rl_chess.inference_server import PredictionClient

class SelfPlayWorker(multiprocessing.Process):
    """
    –ü—Ä–æ—Ü–µ—Å—Å, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ self-play.
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–≥—Ä–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, –∏–≥—Ä–∞—è —Å–∞–º —Å —Å–æ–±–æ–π, –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∏—Ö –≤ ReplayBuffer.
    """
    def __init__(self, worker_id, input_queue, output_queue, replay_buffer: SharedReplayBuffer, total_games_played_counter, shared_inference_buffer, stats_counters):
        super().__init__()
        self.worker_id = worker_id
        self.input_queue = input_queue
        self.output_queue = output_queue
        self.replay_buffer = replay_buffer
        self.total_games_played_counter = total_games_played_counter
        self.shared_inference_buffer = shared_inference_buffer
        self.white_wins, self.black_wins, self.draws = stats_counters
        self.name = f"SelfPlayWorker-{worker_id}"
        self.logger = None

    def run(self):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞–±–æ—á–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞.
        """
        setup_worker_logging() # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ª–æ–≥
        # –°–æ–∑–¥–∞–µ–º –ª–æ–≥–≥–µ—Ä –¥–ª—è live-–∞–ø–¥–µ–π—Ç–æ–≤ –í–ù–£–¢–†–ò –¥–æ—á–µ—Ä–Ω–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
        self.logger = get_live_logger('live_updates.log', f"Player-{self.worker_id}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        predictor = PredictionClient(self.worker_id, self.input_queue, self.output_queue, self.shared_inference_buffer)
        
        # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞ (CPU –¥–ª—è –ª–æ–≥–∏–∫–∏, –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ GPU —á–µ—Ä–µ–∑ —Å–µ—Ä–≤–µ—Ä)
        device = torch.device("cpu")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–≥–µ–Ω—Ç–∞ –±–µ–∑ –º–æ–¥–µ–ª–∏, –Ω–æ —Å predictor
        agent = MCTSAgent(model=None, device=device, num_simulations=config.MCTS_SIMULATIONS, predictor=predictor)
        
        logging.info(f"üöÄ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–ø—É—â–µ–Ω. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —á–µ—Ä–µ–∑ InferenceServer.")

        while True:
            # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –∏–≥—Ä—ã
            with self.total_games_played_counter.get_lock():
                self.total_games_played_counter.value += 1
                current_game_number = self.total_games_played_counter.value
            logging.info(f"--- –ù–∞—á–∞–ª–æ –∏–≥—Ä—ã #{current_game_number} ---")

            # –ü—Ä–æ—Ü–µ—Å—Å –∏–≥—Ä—ã (Self-Play)
            board = chess.Board()
            history = deque([board.copy()], maxlen=config.BOARD_HISTORY_LENGTH)
            game_memory = []
            move_count = 0
            
            while not board.is_game_over(claim_draw=True):
                move_count += 1
                
                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–æ–º–µ—Ä–∞ —Ö–æ–¥–∞
                # –ü–ª–∞–≤–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –¥–ª—è –ª—É—á—à–µ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
                if move_count <= 15:
                    temperature = 1.2
                elif move_count <= 30:
                    temperature = 0.5
                else:
                    temperature = 0.1

                # –ü–æ–ª—É—á–∞–µ–º —Ö–æ–¥ —á–µ—Ä–µ–∑ MCTS (–∑–∞–ø—Ä–æ—Å—ã –∫ —Å–µ—Ç–∏ –ø–æ–π–¥—É—Ç —á–µ—Ä–µ–∑ —Å–µ—Ä–≤–µ—Ä)
                move, policy_target = agent.get_move(board, board_history=history, temperature=temperature, is_self_play=True)
                
                # –ï—Å–ª–∏ –∞–≥–µ–Ω—Ç –Ω–µ —Å–º–æ–≥ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å —Ö–æ–¥, –¥–æ—Å—Ä–æ—á–Ω–æ –∑–∞–≤–µ—Ä—à–∞–µ–º –ø–∞—Ä—Ç–∏—é
                if move is None:
                    logging.warning("–ê–≥–µ–Ω—Ç –Ω–µ —Å–º–æ–≥ –≤—ã–±—Ä–∞—Ç—å —Ö–æ–¥, –∏–≥—Ä–∞ –ø—Ä–µ–∫—Ä–∞—â–µ–Ω–∞.")
                    break
                
                # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∂–¥—ã–π 50-–π —Ö–æ–¥ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏ (–≤ live_updates.log)
                if move_count % 50 == 0:
                    self.logger.info(f"–ò–≥—Ä–∞ #{current_game_number} | –•–æ–¥ #{move_count}: {format_move(move)}")

                state_tensor = board_to_tensor(history, device)
                # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ policy –Ω–∞ CPU –¥–ª—è shared memory
                game_memory.append([state_tensor, policy_target.cpu()])
                board.push(move)
                history.append(board.copy())

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            total_games = self.total_games_played_counter.value
            
            result_str = board.result(claim_draw=True)
            value = 0
            if result_str == "1-0":
                value = 1
                with self.white_wins.get_lock(): self.white_wins.value += 1
            elif result_str == "0-1":
                value = -1
                with self.black_wins.get_lock(): self.black_wins.value += 1
            else:
                with self.draws.get_lock(): self.draws.value += 1

            logging.info(
                f"–ò–≥—Ä–∞ #{total_games} –∑–∞–≤–µ—Ä—à–µ–Ω–∞. "
                f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result_str} ({move_count} —Ö–æ–¥–æ–≤). "
                f"–û–±—â–∏–π —Å—á–µ—Ç: +{self.white_wins.value} -{self.black_wins.value} ={self.draws.value}"
            )
            
            # –ö–∞–∂–¥—ã–π —Ç–µ–Ω–∑–æ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è –Ω—É–∂–Ω–æ –æ—Ç—Å–æ–µ–¥–∏–Ω–∏—Ç—å –æ—Ç –≥—Ä–∞—Ñ–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π –∏ –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –Ω–∞ CPU
            final_game_memory = []
            for i, data in enumerate(game_memory):
                # –¶–µ–Ω–Ω–æ—Å—Ç—å –º–µ–Ω—è–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ö–æ–¥–∞ (1 –¥–ª—è –ø–æ–±–µ–¥–∏—Ç–µ–ª—è, -1 –¥–ª—è –ø—Ä–æ–∏–≥—Ä–∞–≤—à–µ–≥–æ)
                current_value = value if i % 2 == 0 else -value
                state_tensor, policy = data
                final_game_memory.append([state_tensor.detach().cpu(), policy, current_value])

            # –ù–∞–ø—Ä—è–º—É—é –¥–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –æ–±—â–∏–π –±—É—Ñ–µ—Ä
            self.replay_buffer.add(final_game_memory)
            
            time.sleep(config.GAME_INTERVAL) 