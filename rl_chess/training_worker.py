# -*- coding: utf-8 -*-
"""
–ü—Ä–æ—Ü–µ—Å—Å-—Ç—Ä–µ–Ω–µ—Ä (Training Worker).

–≠—Ç–æ—Ç –ø—Ä–æ—Ü–µ—Å—Å –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏.
–û–Ω –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –±–∞—Ç—á–∏ –¥–∞–Ω–Ω—ã—Ö —É ReplayBufferServer, –≤—ã–ø–æ–ª–Ω—è–µ—Ç —à–∞–≥–∏
–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞ GPU –∏ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –≤–µ—Å–∞
–º–æ–¥–µ–ª–∏ –≤ ModelServer.
"""
import multiprocessing
import torch
import torch.optim as optim
import torch.nn.functional as F
import logging
import time
import os
from torch.utils.data import DataLoader, TensorDataset
from torch.nn import MSELoss, CrossEntropyLoss
from torch.cuda.amp import autocast, GradScaler

import rl_chess.config as config
from rl_chess.RL_network import ChessNetwork
from rl_chess.RL_utils import setup_worker_logging
from rl_chess.shared_buffer import SharedReplayBuffer

class TrainingWorker(multiprocessing.Process):
    """
    –ü—Ä–æ—Ü–µ—Å—Å, –æ—Ç–≤–µ—á–∞—é—â–∏–π –∑–∞ –æ–±—É—á–µ–Ω–∏–µ –æ–±—â–µ–π –º–æ–¥–µ–ª–∏.
    """
    def __init__(self, model: ChessNetwork, replay_buffer: SharedReplayBuffer, optimizer, scheduler, training_step_counter: multiprocessing.Value, total_games_played_counter: multiprocessing.Value, stats_counters=None):
        super().__init__()
        self.model = model
        self.replay_buffer = replay_buffer
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.training_step_counter = training_step_counter
        self.total_games_played_counter = total_games_played_counter
        self.name = "TrainingWorker"
        self.scaler = None # –î–ª—è —Å–º–µ—à–∞–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        self.white_wins = None
        self.black_wins = None
        self.draws = None
        if stats_counters is not None:
            self.white_wins, self.black_wins, self.draws = stats_counters

    def run(self):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –∂–∏–∑–Ω–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞.
        """
        setup_worker_logging()
        device = torch.device(config.TRAINING_DEVICE)
        self.model.to(device)

        # <<< –ù–ê–ß–ê–õ–û –ò–ó–ú–ï–ù–ï–ù–ò–ô
        # –û–¢–ö–õ–Æ–ß–ê–ï–ú Mixed Precision, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∞ –ª–æ–º–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ Policy Head
        # use_bfloat16 = (device.type == 'cuda' and torch.cuda.is_bf16_supported())
        use_bfloat16 = False
        
        # GradScaler –Ω—É–∂–µ–Ω –¥–ª—è float16, –Ω–æ –±–µ–∑–æ–ø–∞—Å–µ–Ω –∏ –¥–ª—è bfloat16 (—Ö–æ—Ç—è –∏ –º–µ–Ω–µ–µ –∫—Ä–∏—Ç–∏—á–µ–Ω)
        # self.scaler = GradScaler(enabled=use_bfloat16)
        self.scaler = None # –û—Ç–∫–ª—é—á–∞–µ–º scaler
        
        log_message = f"üöÄ –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–ø—É—â–µ–Ω –Ω–∞ [{device}]. "
        log_message += "–°–º–µ—à–∞–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: –í—ã–∫–ª—é—á–µ–Ω–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è float32) [FORCED FIX]"
        logging.info(log_message)
        # <<< –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–ô

        # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Å–∫–æ–ª—å–∫–æ –¥–∞–Ω–Ω—ã—Ö –±—ã–ª–æ –ø—Ä–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–º –æ–±—É—á–µ–Ω–∏–∏
        last_trained_buffer_size = 0
        training_start_time = time.time()
        last_log_time = time.time()
        
        while True:
            if not self.replay_buffer.is_ready():
                logging.info(f"–û–∂–∏–¥–∞–Ω–∏–µ –Ω–∞–ø–æ–ª–Ω–µ–Ω–∏—è –±—É—Ñ–µ—Ä–∞... ({self.replay_buffer.size.value}/{config.MIN_REPLAY_BUFFER_SIZE})")
                time.sleep(5)
                continue
            
            # RATE LIMITING: –∂–¥—ë–º –ø–æ–∫–∞ –Ω–∞–∫–æ–ø–∏—Ç—Å—è –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ù–û–í–´–• –¥–∞–Ω–Ω—ã—Ö
            # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–∏—Ö –∏ —Ç–µ—Ö –∂–µ –ø–æ–∑–∏—Ü–∏—è—Ö
            current_buffer_size = self.replay_buffer.size.value
            new_samples = current_buffer_size - last_trained_buffer_size
            
            # –¢—Ä–µ–±—É–µ–º –º–∏–Ω–∏–º—É–º TRAIN_BATCH_SIZE –Ω–æ–≤—ã—Ö –ø–æ–∑–∏—Ü–∏–π –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º —à–∞–≥–æ–º
            if new_samples < config.TRAIN_BATCH_SIZE:
                time.sleep(0.5)  # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞—É–∑–∞, —á—Ç–æ–±—ã –Ω–µ —Å–ø–∞–º–∏—Ç—å CPU
                continue
            
            current_step = self.training_step_counter.value
            games_played = self.total_games_played_counter.value
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥
            if time.time() - last_log_time > 30:
                elapsed = time.time() - training_start_time
                games_per_hour = (games_played / elapsed) * 3600 if elapsed > 0 else 0
                steps_per_hour = (current_step / elapsed) * 3600 if elapsed > 0 else 0
                logging.info(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê: –ò–≥—Ä: {games_played} | –®–∞–≥–æ–≤: {current_step} | "
                           f"–ë—É—Ñ–µ—Ä: {current_buffer_size} | "
                           f"–°–∫–æ—Ä–æ—Å—Ç—å: {games_per_hour:.1f} –∏–≥—Ä/—á–∞—Å, {steps_per_hour:.1f} —à–∞–≥–æ–≤/—á–∞—Å")
                last_log_time = time.time()
            
            batch = self.replay_buffer.sample(config.TRAIN_BATCH_SIZE)

            # <<< –ò–ó–ú–ï–ù–ï–ù–ò–ï –ó–î–ï–°–¨
            # –ü–µ—Ä–µ–¥–∞–µ–º —Ñ–ª–∞–≥ use_bfloat16 –≤ –º–µ—Ç–æ–¥ update_network
            self.update_network(batch, device, use_bfloat16)
            # <<< –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á—ë—Ç—á–∏–∫ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
            last_trained_buffer_size = current_buffer_size
            
            if self.training_step_counter.value % config.SAVE_CHECKPOINT_EVERY_N_STEPS == 0:
                self._save_checkpoint()

    def _save_checkpoint(self):
        """
        –ê—Ç–æ–º–∞—Ä–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏, –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Å—á–µ—Ç—á–∏–∫–æ–≤ –¥–ª—è –≤–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.
        """
        try:
            logging.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –Ω–∞ —à–∞–≥–µ {self.training_step_counter.value}...")
            
            # –î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            checkpoint_data = {
                'training_steps': self.training_step_counter.value,
                'games_played': self.total_games_played_counter.value,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
            }

            if self.white_wins is not None and self.black_wins is not None and self.draws is not None:
                checkpoint_data.update({
                    'white_wins': self.white_wins.value,
                    'black_wins': self.black_wins.value,
                    'draws': self.draws.value,
                })

            # –ê—Ç–æ–º–∞—Ä–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: —Å–Ω–∞—á–∞–ª–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª, –ø–æ—Ç–æ–º –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º
            temp_checkpoint_path = config.CHECKPOINT_PATH + ".tmp"
            
            torch.save(checkpoint_data, temp_checkpoint_path)
            # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–∏—Å—Ç—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            torch.save(self.model.state_dict(), config.MODEL_PATH)
            
            os.replace(temp_checkpoint_path, config.CHECKPOINT_PATH)
            logging.info(f"‚úÖ –ß–µ–∫–ø–æ–∏–Ω—Ç –∏ –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ '{config.CHECKPOINT_PATH}' –∏ '{config.MODEL_PATH}'.")

        except Exception as e:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
            if os.path.exists(temp_checkpoint_path):
                os.remove(temp_checkpoint_path)

    def update_network(self, batch, device, use_bfloat16):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —à–∞–≥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –æ–±—â–µ–π –º–æ–¥–µ–ª–∏, –±–ª–æ–∫–∏—Ä—É—è –µ–µ –Ω–∞ –≤—Ä–µ–º—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.
        """
        self.model.train()
        
        states, policy_targets, value_targets = batch
        
        states = states.to(device)
        policy_targets = policy_targets.to(device)
        value_targets = value_targets.to(device)

        self.optimizer.zero_grad()

        # <<< –ì–õ–ê–í–ù–û–ï –ò–ó–ú–ï–ù–ï–ù–ò–ï –ó–î–ï–°–¨
        # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º dtype=torch.bfloat16 –¥–ª—è autocast
        # with torch.autocast(device_type=device.type, dtype=torch.bfloat16, enabled=use_bfloat16):
        # <<< –ö–û–ù–ï–¶ –ò–ó–ú–ï–ù–ï–ù–ò–Ø
        # –°–µ—Ç—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç log_softmax –¥–ª—è –ø–æ–ª–∏—Ç–∏–∫–∏ –∏ tanh –¥–ª—è value
        log_policy_preds, value_preds = self.model(states)
            
        # 1. –ü–æ—Ç–µ—Ä–∏ –∑–Ω–∞—á–µ–Ω–∏—è (Value Loss) - Mean Squared Error
        value_loss = MSELoss()(value_preds.squeeze(), value_targets)
            
        # 2. –ü–æ—Ç–µ—Ä–∏ –ø–æ–ª–∏—Ç–∏–∫–∏ (Policy Loss) - –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º MCTS
        policy_loss = -torch.sum(policy_targets * log_policy_preds) / states.size(0)

        loss = value_loss + policy_loss

        # self.scaler.scale(loss).backward()
        # self.scaler.step(self.optimizer)
        # self.scaler.update()
        
        loss.backward()
        self.optimizer.step()
        
        self.scheduler.step()
        
        with self.training_step_counter.get_lock():
            self.training_step_counter.value += 1
        
        current_step = self.training_step_counter.value
        if current_step % 50 == 0:
            current_lr = self.optimizer.param_groups[0]['lr']
            logging.info(f"üí° –®–∞–≥ {current_step} | Loss: {loss.item():.4f} (v:{value_loss.item():.4f} p:{policy_loss.item():.4f}) | LR: {current_lr:.2e}")
        
        return loss.item()