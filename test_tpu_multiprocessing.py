# -*- coding: utf-8 -*-
"""
–¢–µ—Å—Ç multiprocessing –Ω–∞ TPU.
–ó–∞–ø—É—Å—Ç–∏ –≤ Colab —á—Ç–æ–±—ã –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏.

!pip install torch-xla
!python test_tpu_multiprocessing.py
"""
import os
import time
import multiprocessing as mp

print("=" * 50)
print("–¢–ï–°–¢ MULTIPROCESSING –ù–ê TPU")
print("=" * 50)

# 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ torch_xla
print("\n1. –ü—Ä–æ–≤–µ—Ä–∫–∞ torch_xla...")
try:
    import torch
    import torch_xla
    import torch_xla.core.xla_model as xm
    import torch_xla.distributed.xla_multiprocessing as xmp
    
    device = xm.xla_device()
    print(f"   ‚úÖ torch_xla —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    print(f"   ‚úÖ TPU —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    TPU_OK = True
except ImportError as e:
    print(f"   ‚ùå torch_xla –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
    TPU_OK = False
except Exception as e:
    print(f"   ‚ùå –û—à–∏–±–∫–∞ TPU: {e}")
    TPU_OK = False

# 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ multiprocessing
print("\n2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ multiprocessing...")

def simple_worker(worker_id, result_queue):
    """–ü—Ä–æ—Å—Ç–æ–π –≤–æ—Ä–∫–µ—Ä –±–µ–∑ TPU."""
    import time
    time.sleep(0.5)
    result_queue.put(f"Worker {worker_id} done")

try:
    result_queue = mp.Queue()
    processes = []
    
    for i in range(3):
        p = mp.Process(target=simple_worker, args=(i, result_queue))
        processes.append(p)
        p.start()
    
    for p in processes:
        p.join(timeout=5)
    
    results = []
    while not result_queue.empty():
        results.append(result_queue.get())
    
    print(f"   ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π multiprocessing —Ä–∞–±–æ—Ç–∞–µ—Ç: {results}")
    MP_OK = True
except Exception as e:
    print(f"   ‚ùå –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π multiprocessing –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}")
    MP_OK = False

# 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ multiprocessing —Å TPU (—á–µ—Ä–µ–∑ xmp.spawn)
if TPU_OK:
    print("\n3. –ü—Ä–æ–≤–µ—Ä–∫–∞ xmp.spawn (TPU multiprocessing)...")
    
    def tpu_worker(index):
        """–í–æ—Ä–∫–µ—Ä –¥–ª—è TPU."""
        import torch_xla.core.xla_model as xm
        device = xm.xla_device()
        
        # –ü—Ä–æ—Å—Ç–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –Ω–∞ TPU
        x = torch.randn(100, 100, device=device)
        y = x @ x.T
        xm.mark_step()
        
        print(f"   Worker {index} –Ω–∞ {device}: tensor shape {y.shape}")
    
    try:
        # xmp.spawn –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ –≤—Å–µ—Ö TPU cores
        # nprocs=1 –¥–ª—è –æ–¥–Ω–æ–≥–æ —è–¥—Ä–∞ (v5e-1)
        print("   –ó–∞–ø—É—Å–∫ xmp.spawn —Å 1 –ø—Ä–æ—Ü–µ—Å—Å–æ–º...")
        xmp.spawn(tpu_worker, args=(), nprocs=1, start_method='fork')
        print("   ‚úÖ xmp.spawn —Ä–∞–±–æ—Ç–∞–µ—Ç!")
        XMP_OK = True
    except Exception as e:
        print(f"   ‚ùå xmp.spawn –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}")
        XMP_OK = False
else:
    print("\n3. –ü—Ä–æ–ø—É—Å–∫ —Ç–µ—Å—Ç–∞ xmp.spawn (TPU –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
    XMP_OK = False

# 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ multiprocessing + TPU inference (–Ω–∞—à —Å—Ü–µ–Ω–∞—Ä–∏–π)
if TPU_OK and MP_OK:
    print("\n4. –ü—Ä–æ–≤–µ—Ä–∫–∞: CPU workers + TPU inference...")
    
    def cpu_worker_with_tpu_call(worker_id, model_queue, result_queue):
        """CPU –≤–æ—Ä–∫–µ—Ä –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ TPU."""
        import torch
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ä–∞–±–æ—Ç—É
        for i in range(3):
            # –°–æ–∑–¥–∞—ë–º —Ç–µ–Ω–∑–æ—Ä –Ω–∞ CPU
            x = torch.randn(1, 102, 8, 8)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ "–∏–Ω—Ñ–µ—Ä–µ–Ω—Å" (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –æ—á–µ—Ä–µ–¥—å)
            model_queue.put((worker_id, i, x))
            
            # –ñ–¥—ë–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            time.sleep(0.1)
        
        result_queue.put(f"Worker {worker_id} finished")
    
    def tpu_inference_server(model_queue, num_requests):
        """TPU —Å–µ—Ä–≤–µ—Ä –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞."""
        import torch_xla.core.xla_model as xm
        
        device = xm.xla_device()
        
        # –ü—Ä–æ—Å—Ç–∞—è "–º–æ–¥–µ–ª—å"
        model = torch.nn.Linear(102 * 8 * 8, 100).to(device)
        
        processed = 0
        while processed < num_requests:
            try:
                worker_id, req_id, x = model_queue.get(timeout=2)
                
                # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ TPU
                x_flat = x.view(1, -1).to(device)
                with torch.no_grad():
                    out = model(x_flat)
                xm.mark_step()
                
                processed += 1
                print(f"   TPU processed request from worker {worker_id}, req {req_id}")
            except:
                break
        
        print(f"   TPU server processed {processed} requests")
    
    try:
        model_queue = mp.Queue()
        result_queue = mp.Queue()
        
        num_workers = 2
        requests_per_worker = 3
        total_requests = num_workers * requests_per_worker
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º TPU —Å–µ—Ä–≤–µ—Ä –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
        tpu_process = mp.Process(target=tpu_inference_server, args=(model_queue, total_requests))
        tpu_process.start()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º CPU –≤–æ—Ä–∫–µ—Ä—ã
        workers = []
        for i in range(num_workers):
            p = mp.Process(target=cpu_worker_with_tpu_call, args=(i, model_queue, result_queue))
            workers.append(p)
            p.start()
        
        # –ñ–¥—ë–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        for p in workers:
            p.join(timeout=10)
        tpu_process.join(timeout=10)
        
        print("   ‚úÖ CPU workers + TPU inference —Ä–∞–±–æ—Ç–∞–µ—Ç!")
        HYBRID_OK = True
    except Exception as e:
        print(f"   ‚ùå –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: {e}")
        HYBRID_OK = False
else:
    print("\n4. –ü—Ä–æ–ø—É—Å–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞")
    HYBRID_OK = False

# –ò—Ç–æ–≥–∏
print("\n" + "=" * 50)
print("–ò–¢–û–ì–ò:")
print("=" * 50)
print(f"TPU –¥–æ—Å—Ç—É–ø–µ–Ω:           {'‚úÖ' if TPU_OK else '‚ùå'}")
print(f"Multiprocessing:        {'‚úÖ' if MP_OK else '‚ùå'}")
print(f"xmp.spawn:              {'‚úÖ' if XMP_OK else '‚ùå'}")
print(f"CPU workers + TPU:      {'‚úÖ' if HYBRID_OK else '‚ùå'}")
print("=" * 50)

if HYBRID_OK:
    print("\nüéâ –ú–û–ñ–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å multiprocessing —Å TPU!")
    print("   –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: CPU workers ‚Üí Queue ‚Üí TPU inference server")
elif XMP_OK:
    print("\n‚ö†Ô∏è –¢–æ–ª—å–∫–æ xmp.spawn —Ä–∞–±–æ—Ç–∞–µ—Ç (–Ω—É–∂–Ω–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫–æ–¥–∞)")
elif MP_OK:
    print("\n‚ö†Ô∏è –¢–æ–ª—å–∫–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π multiprocessing (–±–µ–∑ TPU)")
else:
    print("\n‚ùå Multiprocessing –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç")
