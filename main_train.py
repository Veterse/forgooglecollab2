# -*- coding: utf-8 -*-
"""
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π TPU, CUDA –∏ CPU.
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ª—É—á—à–µ–µ –¥–æ—Å—Ç—É–ø–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ.

–ó–∞–ø—É—Å–∫:
    python main_train.py
    python main_train.py --workers 4 --mcts 200
"""
import os
import sys
import time
import logging
import argparse
from collections import deque
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

import torch
import torch.optim as optim
import chess
import numpy as np

# TPU support
try:
    import torch_xla
    import torch_xla.core.xla_model as xm
    TPU_AVAILABLE = True
except ImportError:
    TPU_AVAILABLE = False

from rl_chess.RL_network import ChessNetwork, board_to_tensor
from rl_chess.RL_agent import MCTSAgent
from rl_chess.RL_utils import move_to_index
import rl_chess.config as config

# --- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("training.log", mode='a', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


def get_device():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: TPU > CUDA > CPU."""
    if TPU_AVAILABLE:
        try:
            device = xm.xla_device()
            logger.info(f"‚úÖ TPU –¥–æ—Å—Ç—É–ø–µ–Ω: {device}")
            return device, 'tpu'
        except Exception as e:
            logger.warning(f"TPU –æ—à–∏–±–∫–∞: {e}")
    
    if torch.cuda.is_available():
        device = torch.device('cuda')
        logger.info(f"‚úÖ CUDA –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.get_device_name(0)}")
        return device, 'cuda'
    
    logger.info("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
    return torch.device('cpu'), 'cpu'


def play_game(model, device, mcts_sims=100):
    """–ò–≥—Ä–∞–µ—Ç –æ–¥–Ω—É –ø–∞—Ä—Ç–∏—é self-play —Å –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–º MCTS."""
    agent = MCTSAgent(model, device=device, num_simulations=mcts_sims)
    
    board = chess.Board()
    history = deque([board.copy()], maxlen=config.BOARD_HISTORY_LENGTH)
    game_data = []
    move_count = 0
    
    while not board.is_game_over(claim_draw=True):
        move_count += 1
        
        # –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
        if move_count <= 15:
            temp = 1.2
        elif move_count <= 30:
            temp = 0.5
        else:
            temp = 0.1
        
        move, policy = agent.get_move(board, board_history=history, temperature=temp, is_self_play=True)
        
        if move is None:
            break
        
        state = board_to_tensor(history, device).cpu()
        policy_cpu = policy.cpu() if policy is not None else torch.zeros(4672)
        game_data.append([state, policy_cpu])
        
        board.push(move)
        history.append(board.copy())
    
    # –†–µ–∑—É–ª—å—Ç–∞—Ç
    result = board.result(claim_draw=True)
    value = 1.0 if result == "1-0" else (-1.0 if result == "0-1" else 0.0)
    
    # –î–æ–±–∞–≤–ª—è–µ–º value
    final_data = []
    for i, (state, policy) in enumerate(game_data):
        v = value if i % 2 == 0 else -value
        final_data.append((state, policy, v))
    
    return final_data, result, move_count


def train_step(model, optimizer, states, policies, values, device, device_type):
    """–û–¥–∏–Ω —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è."""
    model.train()
    
    states = states.to(device)
    policies = policies.to(device)
    values = values.to(device)
    
    optimizer.zero_grad()
    
    log_policy, value_pred = model(states)
    
    value_loss = torch.nn.functional.mse_loss(value_pred.squeeze(), values)
    policy_loss = -torch.sum(policies * log_policy) / states.size(0)
    loss = value_loss + policy_loss
    
    loss.backward()
    
    # TPU-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π —à–∞–≥
    if device_type == 'tpu':
        xm.optimizer_step(optimizer)
        xm.mark_step()
    else:
        optimizer.step()
    
    return loss.item(), value_loss.item(), policy_loss.item()


def save_checkpoint(model, optimizer, game_num, step, replay_buffer, stats, path="rl_checkpoint.pth"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —á–µ–∫–ø–æ–∏–Ω—Ç."""
    # –î–ª—è TPU –ø–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ CPU
    if TPU_AVAILABLE:
        model_state = {k: v.cpu() for k, v in model.state_dict().items()}
    else:
        model_state = model.state_dict()
    
    checkpoint = {
        'model_state_dict': model_state,
        'optimizer_state_dict': optimizer.state_dict(),
        'game_number': game_num,
        'training_step': step,
        'replay_buffer': list(replay_buffer)[-20000:],
        'stats': stats
    }
    
    # –ê—Ç–æ–º–∞—Ä–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    temp_path = path + ".tmp"
    torch.save(checkpoint, temp_path)
    os.replace(temp_path, path)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –æ—Ç–¥–µ–ª—å–Ω–æ
    torch.save(model_state, config.MODEL_PATH)
    
    logger.info(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: –∏–≥—Ä–∞ {game_num}, —à–∞–≥ {step}")


def load_checkpoint(model, optimizer, path="rl_checkpoint.pth"):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —á–µ–∫–ø–æ–∏–Ω—Ç."""
    if not os.path.exists(path):
        return 0, 0, deque(maxlen=config.MEMORY_SIZE), {'white': 0, 'black': 0, 'draw': 0}
    
    try:
        checkpoint = torch.load(path, map_location='cpu')
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        replay_buffer = deque(checkpoint.get('replay_buffer', []), maxlen=config.MEMORY_SIZE)
        stats = checkpoint.get('stats', {'white': 0, 'black': 0, 'draw': 0})
        
        logger.info(f"‚úÖ –ß–µ–∫–ø–æ–∏–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: –∏–≥—Ä–∞ {checkpoint['game_number']}, —à–∞–≥ {checkpoint['training_step']}")
        return checkpoint['game_number'], checkpoint['training_step'], replay_buffer, stats
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {e}")
        return 0, 0, deque(maxlen=config.MEMORY_SIZE), {'white': 0, 'black': 0, 'draw': 0}



def main():
    parser = argparse.ArgumentParser(description="–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ RL Chess")
    parser.add_argument("--workers", type=int, default=4, help="–ß–∏—Å–ª–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∏–≥—Ä (threads)")
    parser.add_argument("--mcts", type=int, default=150, help="MCTS —Å–∏–º—É–ª—è—Ü–∏–π")
    parser.add_argument("--batch", type=int, default=256, help="–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞")
    parser.add_argument("--games-per-train", type=int, default=5, help="–ò–≥—Ä –º–µ–∂–¥—É –æ–±—É—á–µ–Ω–∏—è–º–∏")
    parser.add_argument("--save-every", type=int, default=20, help="–°–æ—Ö—Ä–∞–Ω—è—Ç—å –∫–∞–∂–¥—ã–µ N –∏–≥—Ä")
    parser.add_argument("--max-games", type=int, default=100000, help="–ú–∞–∫—Å–∏–º—É–º –∏–≥—Ä")
    args = parser.parse_args()
    
    logger.info("=" * 60)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø RL CHESS")
    logger.info("=" * 60)
    
    # –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
    device, device_type = get_device()
    logger.info(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device} ({device_type})")
    logger.info(f"–í–æ—Ä–∫–µ—Ä–æ–≤: {args.workers}, MCTS: {args.mcts}, Batch: {args.batch}")
    
    # –ú–æ–¥–µ–ª—å
    model = ChessNetwork().to(device)
    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE, weight_decay=config.WEIGHT_DECAY)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
    game_num, train_step, replay_buffer, stats = load_checkpoint(model, optimizer)
    
    logger.info(f"–°—Ç–∞—Ä—Ç —Å –∏–≥—Ä—ã #{game_num + 1}, –±—É—Ñ–µ—Ä: {len(replay_buffer)}")
    logger.info(f"–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: +{stats['white']} -{stats['black']} ={stats['draw']}")
    
    start_time = time.time()
    total_moves = 0
    
    # Lock –¥–ª—è –ø–æ—Ç–æ–∫–æ–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    model_lock = threading.Lock()
    buffer_lock = threading.Lock()
    
    def play_one_game():
        """–ò–≥—Ä–∞–µ—Ç –æ–¥–Ω—É –∏–≥—Ä—É (–¥–ª—è ThreadPoolExecutor)."""
        with model_lock:
            model.eval()
        
        data, result, moves = play_game(model, device, mcts_sims=args.mcts)
        return data, result, moves
    
    try:
        while game_num < args.max_games:
            # === –§–ê–ó–ê 1: Self-Play (–ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ —á–µ—Ä–µ–∑ threads) ===
            games_batch = []
            
            with ThreadPoolExecutor(max_workers=args.workers) as executor:
                futures = [executor.submit(play_one_game) for _ in range(args.games_per_train)]
                
                for future in as_completed(futures):
                    try:
                        data, result, moves = future.result()
                        games_batch.append((data, result, moves))
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –≤ –∏–≥—Ä–µ: {e}")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for data, result, moves in games_batch:
                game_num += 1
                total_moves += moves
                
                # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                if result == "1-0":
                    stats['white'] += 1
                elif result == "0-1":
                    stats['black'] += 1
                else:
                    stats['draw'] += 1
                
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –±—É—Ñ–µ—Ä
                with buffer_lock:
                    replay_buffer.extend(data)
                
                logger.info(f"–ò–≥—Ä–∞ #{game_num}: {result} ({moves} —Ö–æ–¥–æ–≤) | "
                           f"–ë—É—Ñ–µ—Ä: {len(replay_buffer)} | "
                           f"+{stats['white']} -{stats['black']} ={stats['draw']}")
            
            # === –§–ê–ó–ê 2: –û–±—É—á–µ–Ω–∏–µ ===
            if len(replay_buffer) >= config.MIN_REPLAY_BUFFER_SIZE:
                model.train()
                
                # –ù–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è
                num_train_steps = max(1, len(games_batch))
                
                for _ in range(num_train_steps):
                    with buffer_lock:
                        indices = np.random.choice(len(replay_buffer), args.batch, replace=False)
                        batch_data = [replay_buffer[i] for i in indices]
                    
                    states = torch.stack([d[0] for d in batch_data])
                    policies = torch.stack([d[1] for d in batch_data])
                    values = torch.tensor([d[2] for d in batch_data], dtype=torch.float32)
                    
                    loss, v_loss, p_loss = train_step(
                        model, optimizer, states, policies, values, device, device_type
                    )
                    train_step_num = train_step + 1
                    train_step = train_step_num
                    
                    if train_step % 10 == 0:
                        logger.info(f"–®–∞–≥ {train_step} | Loss: {loss:.4f} (v:{v_loss:.4f} p:{p_loss:.4f})")
            
            # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
            if game_num % args.save_every == 0:
                save_checkpoint(model, optimizer, game_num, train_step, replay_buffer, stats)
                
                elapsed = time.time() - start_time
                games_per_hour = game_num / elapsed * 3600 if elapsed > 0 else 0
                avg_moves = total_moves / game_num if game_num > 0 else 0
                
                logger.info(f"üìä {games_per_hour:.1f} –∏–≥—Ä/—á–∞—Å | –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_moves:.0f}")
    
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    
    finally:
        save_checkpoint(model, optimizer, game_num, train_step, replay_buffer, stats)
        logger.info(f"\n{'='*60}")
        logger.info(f"–ò–¢–û–ì–û: {game_num} –∏–≥—Ä, {train_step} —à–∞–≥–æ–≤")
        logger.info(f"–°—á—ë—Ç: +{stats['white']} -{stats['black']} ={stats['draw']}")
        logger.info(f"{'='*60}")


if __name__ == "__main__":
    main()
